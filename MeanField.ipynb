{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "import matplotlib\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import laplace\n",
    "from scipy.optimize import minimize\n",
    "from scipy import integrate\n",
    "from scipy.optimize import minimize\n",
    "from scipy.misc import derivative\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import gamma \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2> Mean Field Approximation Example: </h2>\n",
    "\n",
    "Example 1: The Univariate Gaussian (Example taken from Bishop´s Pattern Recognition Book)\n",
    "\n",
    " Assume we have unidimensional data drawn from a unknown Normal distribution. Suppose we want to estimate the parameters\n",
    " $\\mu$ and $\\sigma$ using a Bayesian approach. Then we have:\n",
    "\n",
    " \\begin{eqnarray}\n",
    "   P(\\mu,\\sigma|D) &=& P(D|\\mu,\\sigma) P(\\mu,\\sigma)\\\\\n",
    "                   &=& P(D|\\mu,\\sigma) P(\\mu|\\sigma)P(\\sigma)\\\\\n",
    " \\end{eqnarray}\n",
    " \n",
    "  Given that $P(D|\\mu,\\sigma)$ is Gaussian, we use conjugate priors for $\\mu$ and $\\sigma$:\n",
    "\n",
    " $P(\\mu|\\sigma) = \\mathcal{N}(\\mu|\\mu_0,(\\lambda_0 \\sigma)^{-1})$\n",
    " \n",
    " $P(\\sigma) = Gamma(\\sigma|a_0,b_0) = \\frac{{b_0}^{a_0}}{\\Gamma(a_0)} \\sigma^{a_0-1} e^{-b_0 \\sigma}$\n",
    "\n",
    "  Together these distributions constitute a Gaussian-Gamma posterior for $P(\\mu,\\sigma|D)$:\n",
    "  \n",
    "  $P(\\mu,\\sigma|m,\\lambda,\\alpha,\\beta) = \\frac{\\beta^\\alpha \\sqrt{\\lambda}}{\\Gamma(\\alpha)\\sqrt{2 \\pi}} \\sigma^{\\alpha - \\frac{1}{2}} e^{-\\beta \\sigma} e^{-\\frac{\\lambda \\sigma (\\mu-m)}{2}}$\n",
    "\n",
    "\n",
    " In this case the posterior can be found exactly and is a Gaussian-Gamma distribution, but for tutorial purposes lets approximate it using variational inference.\n",
    " \n",
    " We will approximate the posterior distribution $q(\\mu,\\sigma)$ as the product of independent distributions:\n",
    " \n",
    "   $q(\\mu,\\sigma) = q_{\\mu}(\\mu)q_{\\sigma}(\\sigma)$\n",
    "   \n",
    " From the mean field equations we know that:\n",
    " \n",
    " \\begin{eqnarray}\n",
    " \\ln q_{\\mu}(\\mu) &=& \\mathbb{E}_{q_{\\sigma}}[\\ln p(D|\\mu,\\sigma) + \\ln p(\\mu|\\sigma)] + const\\\\\n",
    "                     &=& - \\frac{\\mathbb{E}_{q_{\\sigma}}[\\sigma]}{2} \\Big\\{ \\lambda_0 (\\mu - \\mu_0)^2 + \\sum_{n=1}^{N} (x_n - \\mu)^2 \\Big\\} + const \n",
    " \\end{eqnarray}\n",
    " \n",
    " Note that the prior $p(\\sigma)$ is constant for $q_{\\mu}(\\mu)$. Completing the square over $\\mu$ we can see that $q_{\\mu}(\\mu)$ is a Gaussian $\\mathcal{N}(\\mu | \\mu_N, \\lambda_N^{-1} )$\n",
    " where:\n",
    " \n",
    " \\begin{eqnarray}\n",
    "    \\mu_N &=& \\frac{\\lambda_0 \\mu_0 + N \\overline{x}}{\\lambda_0 + N}\\\\\n",
    "    \\lambda_N &=& (\\lambda_0 + N) \\mathbb{E}_{q_{\\sigma}}[\\sigma] \n",
    " \\end{eqnarray}\n",
    "\n",
    " Similarly, the optimal solution for $q_{\\sigma}(\\sigma)$ is:\n",
    " \n",
    " \\begin{eqnarray}\n",
    "  \\ln q_{\\sigma}(\\sigma) &=& \\mathbb{E}_{q_{\\mu}}[\\ln p(D |\\mu, \\sigma) + \\ln p(\\mu|\\sigma) + \\ln p(\\sigma)] + const\\\\\n",
    "                           &=& (a_0 - 1) \\ln\\sigma - b_0\\sigma + \\frac{N}{2} \\ln\\sigma\\\\\n",
    "                     && - \\frac{\\sigma}{2} \\mathbb{E}_{q_{\\mu}} \\Bigg[ \\sum_{n=1}^{N} (x_n - \\mu)^2 + \\lambda_0 (\\mu - \\mu_0)^2 \\Bigg] + const\n",
    " \\end{eqnarray}\n",
    " \n",
    " Hence $q_{\\sigma}(\\sigma)$ is a Gamma distribution $Gamma(\\sigma | a_N,b_N)$ with parameters:\n",
    " \n",
    " \\begin{eqnarray}\n",
    "    a_N &=& a_0 + \\frac{N}{2} \\\\\n",
    "    b_N &=& b_0 + \\frac12 \\mathbb{E}_{q_{\\mu}} \\Bigg[ \\sum_{n=1}^{N} (x_n - \\mu)^2 + \\lambda_0 (\\mu - \\mu_0)^2 \\Bigg]\n",
    " \\end{eqnarray}\n",
    " \n",
    " Using the known value for the expectation of $\\sigma$,we have that $\\mathbb{E}_{q_{\\sigma}}[\\sigma] = \\frac{a_N}{b_N}$.\n",
    "Also to simplify the expressions we can assume non informative priors:\n",
    "\n",
    "$\\lambda_0 = a_0 = b_0 = \\mu_0 = 0$\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\frac{1}{\\mathbb{E}_{q_{\\sigma}}[\\sigma]} &=& \\mathbb{E}_{q_{\\mu}} \\Bigg [ \\frac{1}{N} \\sum_{n=1}^{N}(x_n - \\mu)^2  \\Bigg]\n",
    "                                 &=& \\overline{x^2} - 2\\overline{x}\\mathbb{E}_{q_{\\mu}}[\\mu] + \\mathbb{E}_{q_{\\mu}}[\\mu^2]\n",
    "\\end{eqnarray}\n",
    "\n",
    "Then the first and second moments of $q_{\\mu}(\\mu)$ are (recall that is a Gaussian):\n",
    "\n",
    "$\\mathbb{E}_{q_{\\mu}}[\\mu] = \\overline{x}$\n",
    "\n",
    "$\\mathbb{E}_{q_{\\mu}}[\\mu^2] = \\overline{x}^2 + \\frac{1}{N \\mathbb{E}_{q_{\\sigma}}[\\sigma]}$\n",
    "\n",
    " Substituting with the previous equation we have:\n",
    "\n",
    "$\\mathbb{E}_{q_{\\sigma}}[\\sigma] = \\frac{N-1}{\\sum_{n=1}^{N}(x_n - \\overline{x})^2}$ (Note that this is the inverse of the variance of a univariate Gaussian distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_0 = 0\n",
    "a_0 = 0\n",
    "b_0 = 0\n",
    "mu_0 = 0\n",
    "\n",
    "def calculate_parameters(x,E_mu, E_sigma, a_0 = 0, mu_0 = 0, l_0 = 0):\n",
    "    N = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    mu_N = (l_0*mu_0 + N*x_mean)/(l_0 + N)\n",
    "    l_N_inverse = 1/((l_0 + N)*E_sigma)\n",
    "    a_N = a_0 + N/2\n",
    "    b_N = 1/(E_sigma/a_N)\n",
    "    return mu_N, l_N_inverse, a_N, b_N\n",
    "    \n",
    "def e_q_sigma(x,E_mu,E_sigma):\n",
    "    N = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    x_square_mean = np.mean(sum(x**2))\n",
    "    E_mu_square = x_mean**2 + 1/(N*E_sigma)\n",
    "    return (1/(x_square_mean - 2*x_mean*E_mu + E_mu_square))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.06079672765e-149\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "mu = 3\n",
    "sigma = 1\n",
    "N = 30\n",
    "x1 = np.random.normal(mu, sigma, N)\n",
    "iterations = 10\n",
    "\n",
    "gaussian = lambda x,mu,sigma: (1/(np.sqrt(2 *sigma* np.pi))) *np.exp( - (x - mu)**2 / (2 * sigma)) \n",
    "gamma_pdf = lambda s,a,b: ((b**a)/gamma(a))*(s**(a-1))*np.exp(-1*b*s)\n",
    "\n",
    "E_mu = np.mean(x1)\n",
    "E_sigma = 1\n",
    "x, y = np.mgrid[0:6:.1, 0:1:.1]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "\n",
    "#nx, ny = (3, 2)\n",
    "#x = np.linspace(0, 1, nx)\n",
    "#y = np.linspace(0, 1, ny)\n",
    "#xv, yv = meshgrid(x, y)\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    E_sigma = e_q_sigma(x1,E_mu,E_sigma)\n",
    "    mu_N, l_N_inverse, a_N, b_N = calculate_parameters(x1,E_mu,E_sigma)\n",
    "    #print(mu_N)\n",
    "    #print(l_N_inverse)\n",
    "    #print(a_N)\n",
    "    #print(b_N)\n",
    "    \n",
    "q_mu_sigma = lambda x_,s_: (gaussian(x_,mu_N,l_N_inverse))*(gamma_pdf(s_,a_N,b_N)) \n",
    "integral = sum(sum(q_mu_sigma(x,y)))\n",
    "print(integral)\n",
    "plt.contour(x, y, (q_mu_sigma(x,y)))\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetro de gaussian gamma\n",
    "lambda = N\n",
    "ALPHA = N/2\n",
    "beta = 1/2(sum(x_i**2))\n",
    "m = (1/N)*sum(x_i)\n",
    "P(μ,σ|m,λ,α,β)=βαλ√Γ(α)2π√σα−12e−βσe−λσ(μ−m)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = np.mgrid[0:6:.1, 0:1:.1]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "\n",
    "l1 = len(x1)\n",
    "a1 = l1/2\n",
    "b1= (1/2)*sum(x1**2)\n",
    "m1 = (sum(x1))/l1\n",
    "gaussian_gamma = lambda x_,y_, l, a, b, m : (((b**a)*np.sqrt(l)) / (gamma(a)*np.sqrt((np.pi)*2)))* (y_**(a-(1/2))) * np.exp(-1*b*y_) * np.exp(-(l*y_*(x_-m)**2)/2)\n",
    "gaussian_gamma_plt = lambda x_,y_: gaussian_gamma(x_,y_,l1,a1,b1,m1)\n",
    "#integral2 = sum(sum(gaussian_gamma_plt(x,y))) \n",
    "plt.contour(x, y, gaussian_gamma_plt(x,y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
